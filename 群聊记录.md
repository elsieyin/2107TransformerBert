以下回答基本都来自可可爱爱助教致Geralt

ria：![image-20210730161953815](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730161953815.png)

这个地方相当于是按照slots切片排序，这个属于Python的sort多级排序，给你看个例子![image-20210730162023518](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730162023518.png)
slot_voab重新排下序，然后首选按照slot_vocab每个元素的[2:]排序，如果[2:]相同的话就按照元素的[:2]部分排序

麦克阿瑟：![image-20210730162057591](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730162057591.png)

loss = loss / self.args.gradient_accumulation_steps，请问这句话是不是应该写在那底下啊，应该在step的时候再去平均吧？
不是，就是在外边求，在step那一步是累积梯度求导的，外边这个是对loss进行标准化。
每累积一次都要除嘛？不应该累积gradient_accumulation_steps次才除以它嘛，为什么1每累积一次就除以.gradient_accumulation_steps啊？
不是，这里累积指的是梯度累积，loss的话直接平均就可以，其实这里loss不做平均也可以，这里可以稍微改写下。
那应该怎么改啊，每一次都除的话，也会影响梯度啊
![image-20210730162314557](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730162314557.png)

改成这样的，loss不用平均，因为loss没有累加，还是直接从model(**inputs)算出来的.https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3#file-gradient_accumulation-py

```python
model.zero_grad()                                   # Reset gradients tensors
for i, (inputs, labels) in enumerate(training_set):
    predictions = model(inputs)                     # Forward pass
    loss = loss_function(predictions, labels)       # Compute loss function
    loss = loss / accumulation_steps                # Normalize our loss (if averaged)
    loss.backward()                                 # Backward pass
    if (i+1) % accumulation_steps == 0:             # Wait for several backward steps
        optimizer.step()                            # Now we can do an optimizer step
        model.zero_grad()                           # Reset gradients tensors
        if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...
            evaluate_model()                        # ...have no gradients accumulated
@muaz-git

```

我大概理解啦。。。也就是只要不zero操作，每一次计算loss的时候，之前计算的也会累积到。。也就是计算第n次loss的时候，求backward的时候会计算出1到n的梯度，是这个意思嘛？
对的![image-20210730162449277](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730162449277.png)

![image-20210730162458524](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730162458524.png)

你看这个pytorch的例子就明白了

永三尺：<img src="C:/Users/86182/AppData/Roaming/Typora/typora-user-images/image-20210730155041699.png" alt="image-20210730155041699" style="zoom: 67%;" />

![image-20210730155046494](C:/Users/86182/AppData/Roaming/Typora/typora-user-images/image-20210730155046494.png)

![image-20210730155054132](C:/Users/86182/AppData/Roaming/Typora/typora-user-images/image-20210730155054132.png)

请问这里构建token_type_ids时，全部都是用0来建模，最后的embedding也都是0，那这个token_type_ids有什么意义啊？
是否是第二句话，针对text pari任务而言的，这里每个样本输入只有一个text，所以token type ids都是0，代表所有token都是来自一句话。
如果样本只有一句话，那这个操作也是必须的吗？
不管是一句话还是两句话都是需要的，模型的输入包括两个输入，一个tokenids 一个是token_typeids。
那 position_embedding为什么可以没有啊？
这个不需要，这个是模型的权重参数，不是输入。

麦克阿瑟：bert里1个单词还能拆成多份嘛？这个依照什么规律拆啊？
![image-20210730155509706](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730155509706.png)

WordPiece分词，可以产生subword
把中文的ner用bert会出现这种情况嘛？
不会，中文的字都是单个粒度，英文的一个字是多个字母。
咱们课上讲的关系抽取代码，只能一句话抽取一个三元组嘛？
是的，多句话可以分完句提取。

Nicholas-kai：bert模型的多头是一起训练还是一个个训练？
一起训练的。

左玉辉： pytorch半精度下 想重复反向传播，怎么设置retrain？使用fp16了 想加对抗训练 但是需要多次backward，amp.scale_loss 也是有 retain_graph的



Oliver：这一句为啥多一个.![image-20210730155827496](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730155827496.png)是相对路径导入的意思吗？
对的，当前路路径下有个module
报错没找到包ImportError: attempted relative import with no known parent package，我这里是有包的。![image-20210730155953280](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730155953280.png)
嗯嗯应该是路径有问题了，要不你改写成绝对路径，或者添加到sys.path.append('xxx/model')，from model.module import xxx
 No module named 'model'，可以了 
嗯嗯，执行入口文件也会影响。
这个改成model父级目录地址就好啦。
执行入口文件是啥？
就是你你运行的哪个文件，最好import的时候把路径写完整，比如首先确定好项目的目录，然后import的时候如果报错找不到的话，就把添加sys.path.append('项目目录/导入包的上级目录')
![image-20210730160218370](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730160218370.png)
https://blog.csdn.net/liudinglong1989/article/details/104468495

#### 用args参数怎么debug？

args参数要命令行输入
https://blog.csdn.net/counte_rking/article/details/78837028，使用Pycharm给Python程序传递参数![image-20210730160334813](%E7%BE%A4%E8%81%8A%E8%AE%B0%E5%BD%95.assets/image-20210730160334813.png)

